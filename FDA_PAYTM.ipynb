{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "DECISION TREE"
      ],
      "metadata": {
        "id": "C_CQ_zmXV6xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies (uncomment if needed)\n",
        "# !pip install scikit-learn joblib\n",
        "\n",
        "# Imports\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    StratifiedKFold,\n",
        "    GridSearchCV\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    auc\n",
        ")\n",
        "\n",
        "# 1) Upload & load your data\n",
        "uploaded = files.upload()            # select Loan.csv\n",
        "df = pd.read_csv('Loan.csv')\n",
        "\n",
        "# 2) Prepare features and target\n",
        "X = df.drop(columns=['LoanApproved','ApplicationDate','Age'])\n",
        "y = df['LoanApproved']\n",
        "\n",
        "# 3) Stratified train/test split (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    stratify=y,\n",
        "    test_size=0.20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 4) Build preprocessing + model pipeline\n",
        "cat_feats = X.select_dtypes(include='object').columns.tolist()\n",
        "num_feats = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('ohe', OneHotEncoder(handle_unknown='ignore'), cat_feats)\n",
        "], remainder='passthrough')\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('pre', preprocessor),\n",
        "    ('clf', DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# 5) Hyperparameter grid & CV search\n",
        "param_grid = {\n",
        "    'clf__max_depth':       [3, 5, 8, None],\n",
        "    'clf__min_samples_leaf':[5, 10, 20],\n",
        "    'clf__criterion':       ['gini','entropy']\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "search = GridSearchCV(\n",
        "    pipe,\n",
        "    param_grid,\n",
        "    cv=cv,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "search.fit(X_train, y_train)\n",
        "best_model = search.best_estimator_\n",
        "\n",
        "print(\"Best params:\", search.best_params_)\n",
        "print(\"CV ROC-AUC: %.4f ¬± %.4f\" % (\n",
        "    np.mean(search.cv_results_['mean_test_score']),\n",
        "    np.std(search.cv_results_['mean_test_score'])\n",
        "))\n",
        "\n",
        "# 6) Evaluate on hold-out test set\n",
        "y_pred  = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"Accuracy  : {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision : {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall    : {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1 Score  : {f1_score(y_test, y_pred):.4f}\")\n",
        "print(f\"ROC AUC   : {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "# 7) Plot the confusion matrix nicely\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "labels = ['Not Approved', 'Approved']\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.imshow(cm, interpolation='nearest')\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, cm[i, j], ha='center', va='center')\n",
        "ax.set_xticks(range(len(labels)))\n",
        "ax.set_yticks(range(len(labels)))\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_yticklabels(labels)\n",
        "ax.set_xlabel('Predicted label')\n",
        "ax.set_ylabel('True label')\n",
        "ax.set_title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 8) Plot ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "roc_auc_val = auc(fpr, tpr)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc_val:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# 9) Feature importances\n",
        "ohe = best_model.named_steps['pre'].named_transformers_['ohe']\n",
        "cat_names = ohe.get_feature_names_out(cat_feats)\n",
        "feat_names = list(cat_names) + num_feats\n",
        "importances = best_model.named_steps['clf'].feature_importances_\n",
        "imp_df = pd.Series(importances, index=feat_names).sort_values(ascending=False).head(20)\n",
        "\n",
        "print(\"\\nTop 20 Feature Importances:\")\n",
        "print(imp_df)\n",
        "imp_df.sort_values().plot.barh(figsize=(6, 8))\n",
        "plt.title(\"Top 20 Feature Importances\")\n",
        "plt.show()\n",
        "\n",
        "# 10) Persist model & export predictions\n",
        "joblib.dump(best_model, 'dt_pipeline.joblib')\n",
        "files.download('dt_pipeline.joblib')\n",
        "\n",
        "full_preds = best_model.predict(X)\n",
        "out_df = pd.DataFrame({\n",
        "    'ApplicationDate': df['ApplicationDate'],\n",
        "    'Age'            : df['Age'],\n",
        "    'LoanApprovedFlag': full_preds\n",
        "})\n",
        "out_df.to_csv('Loan_Predictions.csv', index=False)\n",
        "files.download('Loan_Predictions.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Oqvr32OrV8r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "7CZ7BnAqV9CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Importing required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "file_path = r\"C:\\Users\\umar3\\Downloads\\Loan.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 3: Initial inspection\n",
        "print(\"Initial shape of data:\", df.shape)\n",
        "print(df.head())\n",
        "print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
        "\n",
        "# Step 4: Drop ApplicationDate\n",
        "if 'ApplicationDate' in df.columns:\n",
        "    df.drop('ApplicationDate', axis=1, inplace=True)\n",
        "\n",
        "# Step 5: Encode target variable\n",
        "# We assume LoanApproved is binary (Yes/No or 1/0)\n",
        "if df['LoanApproved'].dtype == 'object':\n",
        "    df['LoanApproved'] = df['LoanApproved'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Step 6: Handle missing values\n",
        "# Numeric: Impute with median | Categorical: Impute with most frequent\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
        "\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
        "\n",
        "# Step 7: Encode categorical variables\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Step 8: Define features and target\n",
        "X = df.drop(['LoanApproved', 'RiskScore'], axis=1)\n",
        "y = df['LoanApproved']\n",
        "\n",
        "# Optional: RiskScore can be explored for further models\n",
        "\n",
        "# Step 9: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 10: Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 11: Logistic Regression Model\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "logreg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 12: Predictions\n",
        "y_pred = logreg.predict(X_test_scaled)\n",
        "y_proba = logreg.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Step 13: Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))  # Important for false negatives!\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_proba))\n",
        "\n",
        "# Step 14: Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# Step 15: Classification Report\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 16: ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
        "plt.plot(fpr, tpr, label=\"ROC Curve (AUC = {:.2f})\".format(roc_auc_score(y_test, y_proba)))\n",
        "plt.plot([0,1], [0,1], 'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lFxF4eHRV_O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIGHT GBM"
      ],
      "metadata": {
        "id": "7GRteJZzV_jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, accuracy_score,\n",
        "    mean_squared_error, confusion_matrix\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Load data\n",
        "df = pd.read_csv(\"Loan.csv\")  # Replace with actual path\n",
        "\n",
        "# 2. Parse date and extract date features\n",
        "df[\"ApplicationDate\"] = pd.to_datetime(df[\"ApplicationDate\"])\n",
        "df[\"AppYear\"] = df[\"ApplicationDate\"].dt.year\n",
        "df[\"AppMonth\"] = df[\"ApplicationDate\"].dt.month\n",
        "df[\"AppDayOfWeek\"] = df[\"ApplicationDate\"].dt.dayofweek\n",
        "df.drop(columns=[\"ApplicationDate\"], inplace=True)\n",
        "\n",
        "# 3. Encode categorical columns\n",
        "categorical_cols = [\n",
        "    \"EmploymentStatus\", \"EducationLevel\", \"MaritalStatus\",\n",
        "    \"HomeOwnershipStatus\", \"LoanPurpose\"\n",
        "]\n",
        "for col in categorical_cols:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
        "\n",
        "# 4. Feature set and targets\n",
        "X = df.drop(columns=[\"LoanApproved\", \"RiskScore\"])\n",
        "y_class = df[\"LoanApproved\"]\n",
        "y_reg = df[\"RiskScore\"]\n",
        "\n",
        "# 5. Train-test split\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "    X, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
        "    X, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 6. LightGBM Classifier\n",
        "clf = lgb.LGBMClassifier(\n",
        "    objective=\"binary\",\n",
        "    n_estimators=10000,\n",
        "    learning_rate=0.01,\n",
        "    num_leaves=31,\n",
        "    random_state=42\n",
        ")\n",
        "clf.fit(\n",
        "    X_train_c, y_train_c,\n",
        "    eval_set=[(X_test_c, y_test_c)],\n",
        "    eval_metric=\"auc\",\n",
        ")\n",
        "\n",
        "# 7. Classifier Evaluation\n",
        "y_pred_proba = clf.predict_proba(X_test_c)[:, 1]\n",
        "y_pred_class = clf.predict(X_test_c)\n",
        "\n",
        "print(\"üîπ Classification Metrics:\")\n",
        "print(\"  - AUC Score:\", roc_auc_score(y_test_c, y_pred_proba))\n",
        "print(\"  - Accuracy:\", accuracy_score(y_test_c, y_pred_class))\n",
        "\n",
        "# Confusion Matrix\n",
        "sns.heatmap(confusion_matrix(y_test_c, y_pred_class), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# 8. LightGBM Regressor\n",
        "reg = lgb.LGBMRegressor(\n",
        "    objective=\"regression\",\n",
        "    n_estimators=10000,\n",
        "    learning_rate=0.01,\n",
        "    num_leaves=31,\n",
        "    random_state=42\n",
        ")\n",
        "reg.fit(\n",
        "    X_train_r, y_train_r,\n",
        "    eval_set=[(X_test_r, y_test_r)],\n",
        "    eval_metric=\"rmse\",\n",
        ")\n",
        "\n",
        "# 9. Regression Evaluation\n",
        "y_pred_reg = reg.predict(X_test_r)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "mse = mean_squared_error(y_test_r, y_pred_reg)\n",
        "rmse = np.sqrt(mse)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_r, y_pred_reg))\n",
        "print(\"üîπ Regression Metrics:\")\n",
        "print(\"  - RMSE:\", rmse)\n",
        "\n",
        "# 10. Optional: Feature Importance Plot\n",
        "lgb.plot_importance(clf, max_num_features=15, importance_type=\"gain\", title=\"Top Features - Classifier\")\n",
        "plt.show()\n",
        "\n",
        "lgb.plot_importance(reg, max_num_features=15, importance_type=\"gain\", title=\"Top Features - Regressor\")\n",
        "plt.show()\n",
        "\n",
        "# 7. Classifier Evaluation\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred_proba = clf.predict_proba(X_test_c)[:, 1]\n",
        "y_pred_class = clf.predict(X_test_c)\n",
        "\n",
        "print(\"üîπ Classification Metrics:\")\n",
        "print(\"  - AUC Score:\", roc_auc_score(y_test_c, y_pred_proba))\n",
        "print(\"  - Accuracy:\", accuracy_score(y_test_c, y_pred_class))\n",
        "\n",
        "# ‚ûï Classification Report\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(y_test_c, y_pred_class))\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Compute ROC curve and ROC area\n",
        "fpr, tpr, thresholds = roc_curve(y_test_c, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label=\"Random Guess\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"AUC - ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hzv9Qf8bWArw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CATBOOST - STACKING MODEL"
      ],
      "metadata": {
        "id": "wWN5vS8CWBUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess data\n",
        "df = pd.read_csv('Loan.csv')\n",
        "target = 'RiskScore'\n",
        "cat_cols = df.select_dtypes(include='object').columns\n",
        "for col in cat_cols:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "df = df.fillna(df.median(numeric_only=True))\n",
        "\n",
        "# Feature Engineering\n",
        "df['DisposableIncome'] = df['MonthlyIncome'] - df['MonthlyDebtPayments']\n",
        "df['NegativeDisposableIncome'] = (df['DisposableIncome'] < 0).astype(int)\n",
        "df['CreditStress'] = df['CreditCardUtilizationRate'] * df['MonthlyDebtPayments']\n",
        "df['PaymentBurden'] = df['MonthlyLoanPayment'] / (df['MonthlyIncome'] + 1)\n",
        "df['AssetsToLiabilities'] = df['TotalAssets'] / (df['TotalLiabilities'] + 1)\n",
        "df['NetAssets'] = df['TotalAssets'] - df['TotalLiabilities']\n",
        "df['NetWorthToAssets'] = df['NetWorth'] / (df['TotalAssets'] + 1)\n",
        "df['ExperienceBin'] = pd.qcut(df['Experience'], q=4, labels=False)\n",
        "\n",
        "# Prepare features and targets\n",
        "X = df.drop(['RiskScore', 'LoanApproved', 'ApplicationDate'], axis=1)\n",
        "y = df[['RiskScore', 'LoanApproved']]\n",
        "y = (y > y.median()).astype(int)\n",
        "\n",
        "# Split and scale data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define base and meta models\n",
        "base_learners = [\n",
        "    ('catboost', CatBoostClassifier(verbose=0, iterations=500, depth=6, learning_rate=0.1)),\n",
        "    ('svm', SVC(probability=True, kernel='rbf', C=1.0))\n",
        "]\n",
        "meta_model = LogisticRegression()\n",
        "\n",
        "# Evaluation function\n",
        "def print_metrics(y_true, y_pred_class, title=\"Model Metrics\"):\n",
        "    accuracy = accuracy_score(y_true, y_pred_class)\n",
        "    recall = recall_score(y_true, y_pred_class)\n",
        "    precision = precision_score(y_true, y_pred_class)\n",
        "    f1 = f1_score(y_true, y_pred_class)\n",
        "\n",
        "    print(f\"\\nüìä {title}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Train models and store predictions\n",
        "stack_models = {}\n",
        "predictions = {}\n",
        "\n",
        "for target in y_train.columns:\n",
        "    print(f\"\\nüéØ Training for target: {target}\")\n",
        "    stack = StackingClassifier(estimators=base_learners, final_estimator=meta_model, cv=5)\n",
        "    stack.fit(X_train_scaled, y_train[target])\n",
        "    preds = stack.predict(X_test_scaled)\n",
        "    stack_models[target] = stack\n",
        "    predictions[target] = preds\n",
        "\n",
        "# Print metrics for each target\n",
        "for target in y_test.columns:\n",
        "    print_metrics(y_test[target], predictions[target], title=f\"{target} Model\")\n",
        "\n",
        "# Plot confusion matrices\n",
        "def plot_conf_matrix(ax, y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False)\n",
        "    ax.set_title(title)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "plot_conf_matrix(axes[0], y_test['RiskScore'], predictions['RiskScore'], \"RiskScore\")\n",
        "plot_conf_matrix(axes[1], y_test['LoanApproved'], predictions['LoanApproved'], \"LoanApproved\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# AUC-ROC Scores\n",
        "for target in ['RiskScore', 'LoanApproved']:\n",
        "    if hasattr(stack_models[target], \"predict_proba\"):\n",
        "        probs = stack_models[target].predict_proba(X_test_scaled)[:, 1]\n",
        "        auc = roc_auc_score(y_test[target], probs)\n",
        "        print(f\"üî∏ AUC-ROC Score for {target}: {auc:.4f}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Model for {target} does not support probability prediction.\")\n",
        "\n",
        "# Classification reports\n",
        "for target in ['RiskScore', 'LoanApproved']:\n",
        "    print(f\"\\nüìã Classification Report for {target}:\\n\")\n",
        "    print(classification_report(y_test[target], predictions[target]))\n"
      ],
      "metadata": {
        "id": "5Rhj6yZVWGCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGB MODEL\n"
      ],
      "metadata": {
        "id": "I9ohh3vEZLwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*M. ALI MITHANI 25900*\n",
        "\n",
        "I USED AN XGBOOST MODEL. This is one additional step and newer direction I used for my model. Instead of a test_train_split that is conventionally used, I used Generative AI to create a test data file for me with the name loan_test with 2000 enteries only and without two columns, namely Loan Approval and Risk Score and that was to be used with training data. The reason I did not use the former technique was because dropping two columns from the training data and using it again to test did not seem like the right thing to do. It would also lead to Data Leakage."
      ],
      "metadata": {
        "id": "lITd2pJVZKsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import files\n",
        "\n",
        "# Upload and load your original training dataset\n",
        "uploaded = files.upload()  # Upload \"Loan.csv\"\n",
        "df = pd.read_csv(\"Loan.csv\")\n",
        "\n",
        "# Drop ID if added previously and ensure the correct columns are present\n",
        "if 'ID' in df.columns:\n",
        "    df = df.drop(columns=['ID'])\n",
        "\n",
        "# Drop target columns to isolate feature distributions\n",
        "features_df = df.drop(columns=['LoanApproved', 'RiskScore'])\n",
        "\n",
        "# Encode categorical variables for sampling\n",
        "cat_cols = features_df.select_dtypes(include='object').columns\n",
        "encoders = {}\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    features_df[col] = le.fit_transform(features_df[col].astype(str))\n",
        "    encoders[col] = le  # Save encoder to decode later\n",
        "\n",
        "# Generate synthetic data using sampling with replacement\n",
        "synthetic_data = features_df.sample(n=2000, replace=True, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Decode categorical columns back to original labels\n",
        "for col in cat_cols:\n",
        "    synthetic_data[col] = encoders[col].inverse_transform(synthetic_data[col])\n",
        "\n",
        "# Save the test set (without target columns)\n",
        "synthetic_data.to_csv('Loan_test.csv', index=False)\n",
        "files.download('Loan_test.csv')"
      ],
      "metadata": {
        "id": "S9_wSVocZKLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I loaded all required librarieies and uploaded test and training data files"
      ],
      "metadata": {
        "id": "-CDYhGIzZQh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install xgboost imbalanced-learn --quiet\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from google.colab import files\n",
        "\n",
        "# Upload training and test datasets\n",
        "print(\"Upload Loan.csv (training set)\")\n",
        "train_file = files.upload()\n",
        "\n",
        "print(\"Upload Loan_test.csv (test set)\")\n",
        "test_file = files.upload()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z-I4jE-_ZQ5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_df = pd.read_csv(\"Loan.csv\")\n",
        "test_df = pd.read_csv(\"Loan_test.csv\")\n"
      ],
      "metadata": {
        "id": "p0ALBZkAZTve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns & First 5 rows of trainig dataset\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "vc_peD5MZVkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical summary of the dataset\n",
        "train_df.describe()"
      ],
      "metadata": {
        "id": "KRcsAPscZWwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data types\n",
        "train_df.info()"
      ],
      "metadata": {
        "id": "OmDO-I6IZYDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking for Null values\n",
        "# There are no null values\n",
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "id": "Jk3OPe_UZZcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head() # Columns & First 5 rows of trainig dataset"
      ],
      "metadata": {
        "id": "OFqXENC4ZbF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I first created an ID column to be used in the final submission file, then I dropped the Application Date Columns. ALthough I checked above that no null values existed, as a fully functioning model to be used in any dataset, where there may be null values, I added the dropna code chunk.\n",
        "\n",
        "Then I carried out Feature Engineering using the given columns and created multiple different newer features and even visualised them in a barplot to see which of them are most important. Then I prepared training features, which are Loan Approval and Risk Score and Standardised the features using Standard Scaler as well as handled class imbalances using smote\n",
        "\n",
        "Then I used parameters for the classification part XGBoost that is used for Loan Approval, and Regression for the Risk Score part, I tried tweaking the parameters by increasing and decreasing n estimators and learning rate although keeping them constant with both models, the best possible parameters were n estimators between 500 and 1000 and the learning rate between 0.05 and 0.5.\n",
        "Then I predicted both classification and then regression values and visualized a confusion matrix as well as classification report and downloaded a submission file"
      ],
      "metadata": {
        "id": "e91Ibtd_ZdZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, f1_score\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# Assign IDs\n",
        "test_df['ID'] = range(1, len(test_df) + 1)\n",
        "\n",
        "# Drop ApplicationDate\n",
        "train_df.drop(columns=['ApplicationDate'], inplace=True)\n",
        "test_df.drop(columns=['ApplicationDate'], inplace=True)\n",
        "\n",
        "# Fill missing values (numeric)\n",
        "train_df = train_df.dropna(axis=0, thresh=int(0.8 * train_df.shape[1]))\n",
        "train_df.fillna(train_df.median(numeric_only=True), inplace=True)\n",
        "test_df.fillna(train_df.median(numeric_only=True), inplace=True)  # Use training stats\n",
        "\n",
        "# Encode categorical variables\n",
        "categoricals = train_df.select_dtypes(include='object').columns\n",
        "label_encoders = {}\n",
        "\n",
        "for col in categoricals:\n",
        "    le = LabelEncoder()\n",
        "    train_df[col] = le.fit_transform(train_df[col].astype(str))\n",
        "    test_df[col] = le.transform(test_df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Feature engineering\n",
        "def engineer_features(df):\n",
        "    df['IncomePerDependent'] = df['AnnualIncome'] / (df['NumberOfDependents'] + 1)\n",
        "    df['LoanToIncome'] = df['LoanAmount'] / (df['AnnualIncome'] + 1)\n",
        "    df['DebtToAssets'] = df['TotalLiabilities'] / (df['TotalAssets'] + 1)\n",
        "    df['CreditUtilization'] = df['CreditCardUtilizationRate'] * df['NumberOfOpenCreditLines']\n",
        "    df['MonthlyFreeIncome'] = df['MonthlyIncome'] - df['MonthlyDebtPayments'] - df['MonthlyLoanPayment']\n",
        "    return df\n",
        "\n",
        "train_df = engineer_features(train_df)\n",
        "test_df = engineer_features(test_df)\n",
        "\n",
        "# Visualize feature importance\n",
        "X_vis = train_df.drop(columns=['LoanApproved', 'RiskScore'])\n",
        "y_vis = train_df['LoanApproved']\n",
        "mi = mutual_info_classif(X_vis, y_vis)\n",
        "pd.Series(mi, index=X_vis.columns).sort_values(ascending=False).plot(\n",
        "    kind='bar', figsize=(14, 5), title=\"Feature Importance (Mutual Info)\"\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "# Prepare training features\n",
        "X = train_df.drop(columns=['LoanApproved', 'RiskScore'])\n",
        "y_class = train_df['LoanApproved']\n",
        "y_reg = train_df['RiskScore']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(test_df.drop(columns=['ID']))\n",
        "\n",
        "# Split data\n",
        "X_train_cls, X_val_cls, y_train_cls, y_val_cls = train_test_split(X_scaled, y_class, test_size=0.2, random_state=42)\n",
        "X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(X_scaled, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_cls_bal, y_train_cls_bal = smote.fit_resample(X_train_cls, y_train_cls)\n",
        "\n",
        "clf_params = {\n",
        "    'max_depth': [3, 5],\n",
        "    'learning_rate': [0.05, 0.5],\n",
        "    'n_estimators': [500, 1000],\n",
        "    'subsample': [0.8, 1],\n",
        "    'colsample_bytree': [0.8, 1]\n",
        "}\n",
        "\n",
        "clf_base = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "clf_grid = GridSearchCV(clf_base, clf_params, scoring='f1', cv=3, verbose=1, n_jobs=-1)\n",
        "clf_grid.fit(X_train_cls_bal, y_train_cls_bal)\n",
        "clf = clf_grid.best_estimator_\n",
        "\n",
        "# Model tuning - Regression\n",
        "reg_params = {\n",
        "    'max_depth': [3, 5],\n",
        "    'learning_rate': [0.05, 0.5],\n",
        "    'n_estimators': [500, 1000],\n",
        "    'subsample': [0.8, 1],\n",
        "    'colsample_bytree': [0.8, 1]\n",
        "}\n",
        "\n",
        "reg_base = XGBRegressor(random_state=42)\n",
        "reg_grid = GridSearchCV(reg_base, reg_params, scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=-1)\n",
        "reg_grid.fit(X_train_reg, y_train_reg)\n",
        "reg = reg_grid.best_estimator_\n",
        "\n",
        "# Evaluate classification\n",
        "y_pred_cls = clf.predict(X_val_cls)\n",
        "print(\"Classification Report:\\n\", classification_report(y_val_cls, y_pred_cls))\n",
        "sns.heatmap(confusion_matrix(y_val_cls, y_pred_cls), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Evaluate regression\n",
        "y_pred_reg = reg.predict(X_val_reg)\n",
        "rmse = np.sqrt(mean_squared_error(y_val_reg, y_pred_reg))\n",
        "print(f\"RMSE (RiskScore): {rmse:.2f}\")\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Example:\n",
        "rmse = np.sqrt(mean_squared_error(y_val_reg, y_pred_reg))\n",
        "r2 = r2_score(y_val_reg, y_pred_reg)\n",
        "\n",
        "print(f\"RMSE: {rmse:.2f}\")\n",
        "print(f\"R¬≤ Score: {r2:.2f}\")\n",
        "\n",
        "# Predict on test set\n",
        "test_df['LoanApproved'] = clf.predict(X_test_scaled)\n",
        "test_df['RiskScore'] = reg.predict(X_test_scaled)\n",
        "\n",
        "# Save submission\n",
        "submission = test_df[['ID', 'LoanApproved', 'RiskScore']]\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "files.download('submission.csv')\n"
      ],
      "metadata": {
        "id": "WAOufWCTZd6g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}